{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
      "\n",
      "Sample data after mean imputation:\n",
      "       CRIM         ZN      INDUS  CHAS       NOX     RM        AGE     DIS  \\\n",
      "0  0.006320  18.000000   2.310000   0.0  0.538000  6.575  67.925658  4.0900   \n",
      "1  0.027310   0.000000   7.070000   0.0  0.469000  6.421  78.900000  4.9671   \n",
      "2  0.027290  11.308114   7.070000   0.0  0.554603  7.185  61.100000  4.9671   \n",
      "3  3.860641   0.000000   2.180000   0.0  0.458000  6.998  45.800000  6.0622   \n",
      "4  3.860641   0.000000  11.211645   0.0  0.458000  7.147  54.200000  6.0622   \n",
      "\n",
      "   RAD         TAX    PTRATIO       B      LSTAT  MEDV  \n",
      "0  1.0  296.000000  18.427438  396.90   4.980000  24.0  \n",
      "1  2.0  242.000000  17.800000  396.90   9.140000  21.6  \n",
      "2  2.0  407.217105  17.800000  392.83   4.030000  34.7  \n",
      "3  3.0  222.000000  18.700000  394.63   2.940000  33.4  \n",
      "4  3.0  222.000000  18.700000  396.90  12.774039  36.2  \n",
      "\n",
      "Sample data after median imputation:\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  76.5  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.538  7.185  61.1  4.9671  2.0  330.0   \n",
      "3  0.28392   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.28392   0.0   9.69   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B   LSTAT  MEDV  \n",
      "0     19.0  396.90   4.980  24.0  \n",
      "1     17.8  396.90   9.140  21.6  \n",
      "2     17.8  392.83   4.030  34.7  \n",
      "3     18.7  394.63   2.940  33.4  \n",
      "4     18.7  396.90  11.645  36.2  \n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load dataset with missing values\n",
    "# Boston Housing dataset doesn't have missing values by default, \n",
    "# so let's artificially introduce some for demonstration.\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "df = boston.frame\n",
    "\n",
    "# Artificially introduce missing values randomly for demo\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.rand(*df.shape) < 0.1  # 10% missing values\n",
    "df = df.mask(missing_mask)\n",
    "\n",
    "# Step 2: Identify columns with missing values\n",
    "missing_cols = df.columns[df.isnull().any()]\n",
    "print(f\"Columns with missing values:\\n{missing_cols.tolist()}\")\n",
    "\n",
    "# Step 3: Impute missing values using mean or median\n",
    "# Create two imputers for demonstration\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Impute using mean\n",
    "df_mean_imputed = df.copy()\n",
    "df_mean_imputed[missing_cols] = mean_imputer.fit_transform(df_mean_imputed[missing_cols])\n",
    "\n",
    "# Impute using median\n",
    "df_median_imputed = df.copy()\n",
    "df_median_imputed[missing_cols] = median_imputer.fit_transform(df_median_imputed[missing_cols])\n",
    "\n",
    "print(\"\\nSample data after mean imputation:\")\n",
    "print(df_mean_imputed.head())\n",
    "\n",
    "print(\"\\nSample data after median imputation:\")\n",
    "print(df_median_imputed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

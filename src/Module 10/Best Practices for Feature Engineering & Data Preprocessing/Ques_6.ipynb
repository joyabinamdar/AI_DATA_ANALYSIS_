{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load dataset with missing values\n",
    "# Boston Housing dataset doesn't have missing values by default, \n",
    "# so let's artificially introduce some for demonstration.\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "df = boston.frame\n",
    "\n",
    "# Artificially introduce missing values randomly for demo\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.rand(*df.shape) < 0.1  # 10% missing values\n",
    "df = df.mask(missing_mask)\n",
    "\n",
    "# Step 2: Identify columns with missing values\n",
    "missing_cols = df.columns[df.isnull().any()]\n",
    "print(f\"Columns with missing values:\\n{missing_cols.tolist()}\")\n",
    "\n",
    "# Step 3: Impute missing values using mean or median\n",
    "# Create two imputers for demonstration\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Impute using mean\n",
    "df_mean_imputed = df.copy()\n",
    "df_mean_imputed[missing_cols] = mean_imputer.fit_transform(df_mean_imputed[missing_cols])\n",
    "\n",
    "# Impute using median\n",
    "df_median_imputed = df.copy()\n",
    "df_median_imputed[missing_cols] = median_imputer.fit_transform(df_median_imputed[missing_cols])\n",
    "\n",
    "print(\"\\nSample data after mean imputation:\")\n",
    "print(df_mean_imputed.head())\n",
    "\n",
    "print(\"\\nSample data after median imputation:\")\n",
    "print(df_median_imputed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Step 1: Load Titanic dataset and identify columns with missing values\n",
    "titanic = fetch_openml(name=\"titanic\", version=1, as_frame=True)\n",
    "df = titanic.frame\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "# Step 2: Impute categorical columns using the most frequent value\n",
    "# Select categorical columns (dtype 'category' or 'object')\n",
    "categorical_cols = df.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Columns among these that have missing values\n",
    "cat_missing_cols = [col for col in categorical_cols if df[col].isnull().any()]\n",
    "\n",
    "print(\"\\nCategorical columns with missing values:\")\n",
    "print(cat_missing_cols)\n",
    "\n",
    "# Initialize imputer with 'most_frequent' strategy\n",
    "most_frequent_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute missing values in categorical columns\n",
    "df_imputed = df.copy()\n",
    "df_imputed[cat_missing_cols] = most_frequent_imputer.fit_transform(df_imputed[cat_missing_cols])\n",
    "\n",
    "print(\"\\nSample data after imputation:\")\n",
    "print(df_imputed[cat_missing_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

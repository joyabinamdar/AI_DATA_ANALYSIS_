{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mydata_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mge\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# === Step 1: Pandas Profiling ===\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_profile_report\u001b[39m(csv_path, subset_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations.dataset'"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import great_expectations as ge\n",
    "from great_expectations.dataset import PandasDataset\n",
    "\n",
    "# === Step 1: Pandas Profiling ===\n",
    "def generate_profile_report(csv_path, subset_columns=None):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if subset_columns:\n",
    "            df = df[subset_columns]\n",
    "\n",
    "        profile = ProfileReport(\n",
    "            df,\n",
    "            title=\"Data Profiling Report\",\n",
    "            explorative=True,\n",
    "            correlations={\"pearson\": {\"calculate\": True}},\n",
    "        )\n",
    "        profile.to_file(\"data_profile_report.html\")\n",
    "        print(\"✔️ Profile report generated successfully: data_profile_report.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in generating profile report: {e}\")\n",
    "\n",
    "# === Step 2: Great Expectations Validation ===\n",
    "class CustomDataset(PandasDataset):\n",
    "    @ge.MetaPandasDataset.column_map_expectation\n",
    "    def expect_column_values_to_be_positive(self, column):\n",
    "        return column > 0\n",
    "\n",
    "def validate_with_expectations(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        dataset = CustomDataset(df)\n",
    "\n",
    "        # Basic Expectations\n",
    "        dataset.expect_column_to_exist(\"age\")\n",
    "        dataset.expect_column_values_to_not_be_null(\"age\")\n",
    "        dataset.expect_column_values_to_be_between(\"age\", min_value=0, max_value=120)\n",
    "        dataset.expect_column_values_to_be_positive(\"age\")\n",
    "\n",
    "        results = dataset.validate()\n",
    "        print(\"✔️ Validation Results:\")\n",
    "        for res in results['results']:\n",
    "            print(f\"{res['expectation_config']['expectation_type']} => {res['success']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation error: {e}\")\n",
    "\n",
    "# === Usage ===\n",
    "csv_file = \"data.csv\"  # Replace with your actual CSV file path\n",
    "columns_to_profile = [\"age\", \"salary\", \"experience\"]  # Adjust as needed\n",
    "\n",
    "generate_profile_report(csv_file, subset_columns=columns_to_profile)\n",
    "validate_with_expectations(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
